import csv
import json
import os
import random
import pandas as pd
import torch
from config import repo_dir
from torch.utils.data import DataLoader, random_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, load_metric
from utils.miscellaneous import checkout_version, get_vulnerable_commit, git_clone
from utils.parse_java import parse_java

def read_dataset(dataset):
    ret = []
    if dataset == 'data/xxx data - cve with link.csv' or dataset== 'data/xxx data - filtered_cve_with_link.csv':
        with open(dataset) as f:
            reader = csv.reader(f)
            next(reader, None)
            for i, line in enumerate(reader):
                repo = line[2].split('https://github.com/')[-1].split('/')[1]
                ret.append({'cve': line[0],
                            'commit_url': line[2],
                            'repo': repo,
                            'version': line[7].split('|')[-1],
                            'versions':line[7].split('|'),
                            'description': line[3],
                            'clazz': line[8],
                            'methods': line[9],
                            'repo_link': line[2].split('commit/')[0],
                            'same_location': line[14]
                            })
    elif dataset=='xxx data - selected.csv':
        with open(dataset) as f:
            reader = csv.reader(f)
            next(reader, None)
            for i, line in enumerate(reader):
                ret.append({'cve': line[0],
                            'repo': line[9],
                            'version': line[11].split('|')[-1],
                            'versions':line[11].split('|'),
                            'description': line[14],
                            'clazz': line[3],
                            'methods': line[4],
                            'repo_link': line[1]
                            })
    else:
         with open(dataset) as f:
            reader = csv.reader(f)
            next(reader, None)
            for i, line in enumerate(reader):
                if line[2]:
                    repo = line[2].split('https://github.com/')[-1].split('/')[1]
                else:
                    repo = line[1].split('/')[-1]
                ret.append({'cve': line[0],
                            'commit_url': line[2],
                            'repo': repo,
                            'version': line[7].split('|')[-1],
                            'versions':line[7].split('|'),
                            'description': line[3],
                            'clazz': line[8],
                            'methods': line[9],
                            'repo_link': line[2].split('commit/')[0],
                            'same_location': line[14]
                            })
    return ret


def read_positive_files():
    with open('with_patch_results/target_files_from_patch.json', 'r') as f:
        targets = json.load(f)
    with open('with_patch_results/original_patch_results.json', 'r') as f:
        original = json.load(f)
    ret = {}
    dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
    for each in dataset:
        cve = each['cve']
        ret[cve] = []
        if cve not in targets:
            continue
        ret[cve].extend(targets[cve])
        ret[cve].extend(original[cve])
        if len(ret[cve]) == 0:
            print('no files for:', cve) 
    return ret


def get_all_files(repo):
    all_files = []
    for root, dir, fs in os.walk(os.path.join(repo_dir, repo)):
        for f in fs:
            if not f.endswith('.java') or 'test' in root or 'Test' in root or '/mock/' in root:
                continue
            file_path = os.path.join(root, f)
            all_files.append(file_path)
    print('list to do', len(all_files))
    return all_files


def save_negative_data(rate:int=10):
    target_files = read_positive_files()
    print(len(target_files.keys()))
    if os.path.exists('with_patch_results/negative_files.json'):
        results = json.load(open('with_patch_results/negative_files.json', 'r'))
    else:
        results = {}
    for i, data in enumerate(read_dataset('data/xxx data - filtered_cve_with_link.csv')):
        cve = data['cve']
        if cve not in target_files:
            continue
        description = data['description']
        print(i, '########## Current cve:', cve)
        files = target_files[cve]
        repo = data['repo']
        repo_path = os.path.join(repo_dir,repo)
        version = data['version']
        if data['same_location'] != '':
            continue
        if not os.path.exists(repo_path) or repo=='jetty.project':
            print('cloning', repo)
            git_clone(repo, data['repo_link'])

        if not os.path.exists(repo_path):
            continue
        
        output_file_path = 'patches/'+cve+'.patch'
        commit_url = data['commit_url']
        version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
        checkout_version(repo_path, version, cve)

        files = get_all_files(repo)

        negative_files = list(set(files) - set(target_files[cve]))
        random.shuffle(negative_files)
        negative_files = negative_files[:len(target_files[cve])*rate]
        results[cve] = negative_files
        # print('Neg files length:', len(negative_files), negative_files[0])
        with open('with_patch_results/negative_files.json', 'w') as f:
            json.dump(results, f)

def read_negative_files():
    with open('with_patch_results/negative_files.json', 'r') as f:
        return json.load(f)

def prepare_data():
    os.makedirs('fine_tune', exist_ok=True)
    if not os.path.exists('fine_tune/prepare_data.json'):
        print('preparing data')
        positive_files = read_positive_files()
        negative_files = read_negative_files()
        print('positive files:', len(positive_files.keys()))
        print('negative files:', len(negative_files.keys()))
        dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
        all_data = {}
        all_data['description'] = []
        all_data['file_content'] = []
        all_data['labels'] = []
        for i, data in enumerate(dataset):
            cve = data['cve']
            if i % 100 == 0:
                print(i, '########## Current cve:', cve)
            

            repo = data['repo']
            repo_path = os.path.join(repo_dir,repo)
            version = data['version']
            if not os.path.exists(repo_path) or repo=='jetty.project':
                print('cloning', repo)
                git_clone(repo, data['repo_link'])

            if not os.path.exists(repo_path):
                continue
            output_file_path = 'patches/'+cve+'.patch'
            commit_url = data['commit_url']
            version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
            checkout_version(repo_path, version, cve)

            if cve in positive_files:

                for file in positive_files[cve]:
                    if not os.path.exists(file):
                        continue
                    try:
                        parsed = parse_java(file, file.split('/')[-1].replace('.java', ''))
                        if not parsed:
                            continue
                        all_data['file_content'].append(parsed['classes'][0]['body'])
                        all_data['description'].append(data['description'])
                        all_data['labels'].append(1)
                    except:
                        print('error:', file)
            if cve in negative_files:
                for file in negative_files[cve]:
                    if not os.path.exists(file):
                        continue
                    try:
                        parsed = parse_java(file, file.split('/')[-1].replace('.java', ''))
                        if not parsed:
                            continue
                        all_data['file_content'].append(parsed['classes'][0]['body'])
                        all_data['description'].append(data['description'])
                        all_data['labels'].append(0)
                    except:
                        print('error:', file)
            print('All data lenght:', len(all_data['description']))
        with open('fine_tune/prepare_data.json', 'w') as f:
            json.dump(all_data, f)
    else:
        with open('fine_tune/prepare_data.json', 'r') as f:
            all_data = json.load(f)
    return all_data



    # train_data = {
    #     "description": ["CVE description 1", "CVE description 2"],
    #     "file_content": ["java code snippet 1", "java code snippet 2"],
    #     "labels": [1, 0]  # 1 for relevant, 0 for not relevant
    # }
    # val_data = {
    #     "description": ["CVE description 3", "CVE description 4"],
    #     "file_content": ["java code snippet 3", "java code snippet 4"],
    #     "labels": [1, 0]
    # }


truncated_cases_count = 0
total_count = 0
def fine_tune():
    import os
    # os.environ["CUDA_VISIBLE_DEVICES"] = "7"
    # Check the total number of GPUs available
    num_gpus = torch.cuda.device_count()
    print(f"Total GPUs available: {num_gpus}")
    if not os.path.exists('fine_tune/train_dataset') and not os.path.exists('fine_tune/val_dataset'):

        all_data = prepare_data()
        # Truncate all_data to first 3k
        for key in all_data:
            all_data[key] = all_data[key]
        # Split the data into training and validation sets with 8:2
        all_indexes = list(range(len(all_data['description'])))
        train_size = int(0.8 * len(all_indexes))
        val_size = len(all_indexes) - train_size
        train_indexes, val_indexes = random_split(all_indexes, [train_size, val_size])
        train_data = {}
        val_data = {}
        for key in all_data:
            train_data[key] = [all_data[key][i] for i in train_indexes]
            val_data[key] = [all_data[key][i] for i in val_indexes]
        



        train_df = pd.DataFrame(train_data)
        val_df = pd.DataFrame(val_data)

        def tokenize_function(examples):
            # Tokenize the inputs (ensuring to return a dictionary with the keys being the column names)
            return tokenizer(examples["description"], examples["file_content"], padding="max_length", truncation=True, return_tensors="np")

        def inspect_truncated_tokens(examples):
            # Concatenate the texts with a separator token
            texts = [desc + tokenizer.sep_token + content for desc, content in zip(examples["description"], examples["file_content"])]
            
            # Tokenize the combined text without truncation
            tokenized_outputs = tokenizer(texts, padding=False, truncation=False, return_tensors=None)
            
            max_length = tokenizer.model_max_length  # Get the max length the model can handle
            
            for i, input_ids in enumerate(tokenized_outputs['input_ids']):
                total_count +=1
                if len(input_ids) > max_length:
                    truncated_cases_count+=1
                    # Identify the tokens that would be truncated
                    truncated_tokens = input_ids[max_length:]
                    # Decode them to see what they are
                    truncated_text = tokenizer.decode(truncated_tokens)
                    # print(f"Example {i} has {len(input_ids) - max_length} truncated tokens: {truncated_text}")
                # else:
                    # print(f"Example {i} is within the model's max length limit.")
                    print(truncated_cases_count, '/', total_count)
        tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')

        # Convert DataFrames to Hugging Face Dataset
        train_dataset = Dataset.from_pandas(train_df)
        val_dataset = Dataset.from_pandas(val_df)

        # Apply the tokenizer to the datasets
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['description', 'file_content'])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['description', 'file_content'])

        # train_dataset = train_dataset.rename_column("labels", "label")
        # val_dataset = val_dataset.rename_column("labels", "label")

        # Set format for PyTorch
        # train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
        # val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

    #     train_dataset.save_to_disk('fine_tune/train_dataset')
    #     val_dataset.save_to_disk('fine_tune/val_dataset')
    else:
        train_dataset = Dataset.load_from_disk('fine_tune/train_dataset')
        val_dataset = Dataset.load_from_disk('fine_tune/val_dataset')
    exit()
    
    model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)
    # Force the device to be CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device = torch.device("cpu")
    print('Using', device)

    if device.type == 'cuda':
        torch.cuda.set_device(0) # Set the CUDA device, if multiple devices are available
        total_memory = torch.cuda.get_device_properties(0).total_memory
        available_memory = total_memory - torch.cuda.memory_allocated(0)
        print(f"Total Memory: {total_memory / (1024**3):.2f} GB")
        print(f"Available Memory: {available_memory / (1024**3):.2f} GB")
    else:
        print("CUDA is not available. Check your installation or if your device supports CUDA.")
    model.to(device)  # Move the model to the CPU
    training_args = TrainingArguments(
        output_dir='./my_model',
        num_train_epochs=3,
        per_device_train_batch_size=42,
        # gradient_accumulation_steps=4,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        evaluation_strategy="epoch",  # Change to evaluate at the end of each epoch
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    trainer.train()
    os.makedirs("./my_model/", exist_ok=True)
    model_save_path = "./my_model/"
    trainer.save_model(model_save_path)
    evaluation_results = trainer.evaluate()
    print(evaluation_results)