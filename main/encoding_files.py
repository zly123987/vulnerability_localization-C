import base64
import io
from tqdm import tqdm
import torch

from config import repo_dir, mongodb_host, mongodb_port, mongo_db
import os
import concurrent.futures
import pymongo
from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel
from utils.parse_java import parse_java
from utils.parse_pom_gav import parse_pom
worker_num = 10

def initialize_model(choice = 'bert'):
    model_name = "microsoft/codebert-base"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = RobertaTokenizer.from_pretrained(model_name)
    if choice == 'bert':
        model = RobertaForSequenceClassification.from_pretrained(model_name).to(device)
    else:
        model = RobertaModel.from_pretrained(model_name).to(device)
    return tokenizer, model, device

def encode_files_gpu(repo, cve, version, tokenizer, model, device, c):
    coll = c[mongo_db]['repo_vectors']
    coll.create_index('id')
    gav_dict = {}
    list_to_do = list()
    for root, dir, fs in os.walk(os.path.join(repo_dir, repo)):
        if os.path.exists(os.path.join(root, 'pom.xml')):
            file_path = os.path.join(root, 'pom.xml')
            data = parse_pom(file_path)
            if data:
                gav = data.get('groupId') + '|' + data.get('artifactId') + '|' + data.get('version')
                gav_dict[root] = gav
            else:
                gav = repo+'|'+repo+'|'+version
                gav_dict[root] = gav

        for f in fs:
            if not f.endswith('.java') or not '/src/main/java/' in root:
                continue
            file_path = os.path.join(root, f)
            # get gav
            if root.split('/src/main/java/')[0] in gav_dict:
                gav = gav_dict.get(root.split('/src/main/java/')[0])
            else:
                gav = repo+'|'+repo+'|'+version
            # get class
            clazz = f.replace('.java', '')

            # get id, skip visited
            id = gav + '@' + clazz
            if coll.find_one({'id': id}):
                continue

            file_sig = id + '&&' + file_path + '&&' + gav + '&&' + repo +'&&' + cve
            list_to_do.append(file_sig)

    list_to_do = [(sentence, tokenizer, model, device) for sentence in list_to_do]
    print('list to do for', cve+':', len(list_to_do))
    for each in tqdm(list_to_do):
        one_file(each)


def encode_files(repo, cve, version, tokenizer, model, device, c):
    coll = c[mongo_db]['repo_vectors']
    tf_idf_coll = c[mongo_db]['tf-idf']
    tf_idf_doc = tf_idf_coll.find_one({'cve': cve})

    coll.create_index('id')
    gav_dict = {}
    list_to_do = list()
    for root, dir, fs in os.walk(os.path.join(repo_dir, repo)):
        if os.path.exists(os.path.join(root, 'pom.xml')):
            file_path = os.path.join(root, 'pom.xml')
            data = parse_pom(file_path)
            if data:
                gav = data.get('groupId', 'g')+'|'+data.get('artifactId', repo)+'|'+data.get('version', 'v')
                gav_dict[root] = gav
            else:
                gav = repo+'|'+version
          
        for f in fs:
            if not f.endswith('.java') or 'test' in root or 'Test' in root:
                continue
            file_path = os.path.join(root, f)

            # not in tf-idf filtered then skip
            if file_path not in tf_idf_doc['path']:
                continue


            # get gav
            gav = None
            for each in gav_dict:
                if each in root:
                    gav = gav_dict.get(each)

            # get class
            clazz = f.replace('.java', '')

            # get id, skip visited
            if not gav:
                gav = repo+'|'+version
            id = gav+'@'+clazz  
            if coll.find_one({'id': id}):
                continue

            file_sig = id + '&&' + file_path + '&&' + gav + '&&' + repo +'&&' + cve
            list_to_do.append(file_sig)

    list_to_do = [(sentence, tokenizer, model, device) for sentence in list_to_do]
    print('list to do for', cve+':', len(list_to_do))
    # initialize_model()
    with concurrent.futures.ProcessPoolExecutor(max_workers=worker_num) as executor:
        executor.map(one_file, list_to_do)




def one_file(args):
    try:
        sentence, tokenizer, model, device = args
        sig = sentence
        c = pymongo.MongoClient(mongodb_host,
                                port=mongodb_port)
        coll = c[mongo_db]['repo_vectors']

        id, file_path, gav, repo, cve = sig.split('&&')

        # g, a, _ = gav.split('|')
        if not os.path.exists(file_path):
            return
        parsed = parse_java(file_path, id.split('@')[-1])
        parsed = encode_parsed(parsed, tokenizer, model)
        version = gav.split('|')[-1]
        doc = {
            'id': id,
            'repo': repo,
            'gav': gav,
            'cve': cve,
            'class': id.split('@')[-1],
            # 'sum': sum
            'meta': parsed,
            'version': version,
            'repo': repo
        }

        coll.update_one({'id': id}, {'$set': doc}, upsert=True)
        # print(coll.count_documents({}) - current_doc_num, '/', all_count, id)
    except Exception as e:
        print('Exception:', e)
        try:
            if 'body' in parsed:
                del parsed['body']
            if 'classes' in parsed:
                del parsed['classes']
            # for cls in parsed['classes']:
            #     if 'body' in cls:
            #         del cls['body']
            #     for method in cls['methods']:
            #         if 'body' in method:
            #             del method['body']
            doc = {
                'id': id,
                'repo': repo,
                'gav': gav,
                'cve': cve,
                'class': id.split('@')[-1],
                # 'sum': sum
                'meta': parsed,
                'version': version,
                'repo': repo,
                'oversize': True
            }
            coll.update_one({'id': id}, {'$set': doc}, upsert=True)
        except:
            doc = {
                'id': id,
                'repo': repo,
                'gav': gav,
                'cve': cve,
                'class': id.split('@')[-1],
                'repo': repo,
                'version': version,
                'oversize': True
            }
            coll.update_one({'id': id}, {'$set': doc}, upsert=True)           
    c.close()


def convert_to_tensor(base64_str):
    # Convert base64 string back to bytes
    decoded_bytes = base64.b64decode(base64_str)

    # Load tensor from bytes
    tensor_buffer = io.BytesIO(decoded_bytes)
    loaded_tensor = torch.load(tensor_buffer)

    return loaded_tensor  # The original tensor is recovered


def convert_to_string(tensor):
    # Convert tensor to bytes
    buffer = io.BytesIO()
    torch.save(tensor, buffer)
    bytes_data = buffer.getvalue()

    # Convert bytes to base64 string
    base64_str = base64.b64encode(bytes_data).decode("utf-8")
    return base64_str


def encode_parsed(parsed, tokenizer, model):
    for cls in parsed['classes']:
        if cls['isInterface']==True or 'abstract' in cls['modifiers']:
            continue
        # if cls['comment']=='':
        cls['sum'] = convert_to_string(bert_encode(cls['body'], tokenizer, model))
        for met in cls['methods']:
            if met['name'].startswith('get') or met['name'].startswith('set') or met['name'].startswith('is'):
                continue
            met['sum'] = convert_to_string(bert_encode(met['body'], tokenizer, model))

    return parsed