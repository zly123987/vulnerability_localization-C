import csv
import json
import torch
import pymongo
import pandas as pd
import re
import os
from config import mongodb_host, mongodb_port, repo_dir, black_list, mongo_db, data_file, final_results_coll, query_expansion_choice, re_rank_approach_choice, retrieval_approach_choice, dataset
from main.encoding_files import encode_files, convert_to_tensor, encode_files_gpu, initialize_model
from main.fine_tune import read_dataset
from main.fine_tune_function import get_all_funcs
from scripts.Lmethod import find_elbow_Lmethod
from scripts.elbow import find_elbow
# from scripts.generate_comments_for_a_repo import one_file
from scripts.natural_language_compare import nl_compare
from tqdm import tqdm
from utils.bert_similarity import calculate_bert
from utils.bm25_similarity import re_rank_bm25
from utils.cosine_similarity import cosine_sim
from utils.enrich_description import enrich_description, get_all_description, get_cwe_description, query_expansion, query_expansion_one
from utils.evaluation import inference_model_function_parrallel
from utils.knowledge_graph import kg_sim, weighted_kg_sim
from utils.miscellaneous import checkout_version, get_vulnerable_commit, normalize_file_path, read_mongodb_file_properties, git_clone
from utils.no_rerank import skip_rerank
from utils.parse_pom_gav import parse_pom
from utils.query_reformulation import reformulate_query
from utils.repo_file_utils import read_filtered_files, read_filtered_funcs, read_patched_files_only
from utils.tf_idf_similarity import calculate_tfidf_cosine_similarity, tokenize_description, calculate_tf_idf_vectors, calculate_tf_idf_vectors_func
import concurrent.futures
from multiprocessing import set_start_method

from utils.word2vec import word_2_vec_sim
debug_cve_id = ''


def preprocess():
    '''
    outdated
    '''
    writer = csv.writer(open('data/results_pre_filter.csv', 'w'))
    c = pymongo.MongoClient(mongodb_host,
                    port=mongodb_port)
    os.makedirs('data/tf-idf', exist_ok=True)
    coll = c[mongo_db]['tf-idf']
    with open(data_file) as f:
        reader = csv.reader(f)
        next(reader, None)
        for line in reader:
            cve = line[0]
            if debug_cve_id not in cve:
                continue
            # if coll.find_one({'cve':cve}):
            #     continue
            cwe = line[7]
            cwe_description = get_cwe_description()[cwe]
            repo = line[9]
            version = line[11].split('|')[-1]
            cwd = os.getcwd()
            if line[14]=='':
                continue
            description = line[14]+' '+line[15]
            target_clss = line[3]
            if '|' in target_clss:
                target_clsses = target_clss.split('|')
            else:
                target_clsses = None

            # if not initialized, then calculate it
            print('='*5, cve, repo, '='*5)
            # if not coll.find_one({'cve':cve}):
            os.chdir(os.path.join(repo_dir, repo))
            os.system(f'git checkout {version} >/dev/null 2>&1')
            os.chdir(cwd)

            description = query_expansion(description, cve, cwe_description)
            calculate_tf_idf_vectors(repo, c, description, cve, version)
            correct = False
            
            top_k = coll.find_one({'cve': cve})['top-k']
            tops = list()

            for k in top_k:
                clss = k.split('@')[-1]
                tops.append(clss)
            if target_clsses:
                for t in target_clsses:
                    if t in tops:
                        print('[INFO] Found in top', tops.index(t))
                        correct= True
                    else:
                        print('[INFO] Not found', t)
                print('\n\n')
            else:
                if target_clss in tops:
                    correct= True
                    print('[INFO] Found in top', tops.index(target_clss), '\n\n')
                else:
                    print('[INFO] Not found', target_clss, '\n\n')

            writer.writerow([cve, repo, correct])
            # break
    c.close()
    

# def generate_summaries(list_to_do, current_doc_num):
#     args_list = [(current_doc_num, len(list_to_do), s) for s in list_to_do]
#     print('list to do NL:', len(list_to_do))
#     for arg in args_list:
#         one_file(arg)
    # with concurrent.futures.ProcessPoolExecutor(4) as executor:
        # executor.map(one_file, args_list)

# def natural_language_filter():
#     writer = csv.writer(open('data/results.csv', 'w'))
#     c = pymongo.MongoClient(mongodb_host,
#                     port=mongodb_port)
#     os.makedirs('data/nl', exist_ok=True)
#     coll = c[mongo_db]['tf-idf']
#     repo_sum = c[mongo_db]['repo_summary_nl_filter']
#     nl_result_coll = c[mongo_db]['nl_filter']
#
#     all_cases = [0, 0]
#     with open(data_file) as f:
#         reader = csv.reader(f)
#         next(reader, None)
#         for line in reader:
#             cve = line[0]
#             repo = line[9]
#             if debug_cve_id not in cve:
#                 continue
#
#             # statistics correctness
#             all_cases[1]+=1
#             correct = False
#
#             # if os.path.exists(f'data/{repo}_nl.csv') and pd.read_csv(f'data/{repo}_nl.csv').size>0:
#             #     continue
#             version = line[11].split('|')[-1]
#             cwd = os.getcwd()
#             description = tokenize_description(line[14]+' '+line[15])
#             target_clss = line[3]
#             if '|' in target_clss:
#                 target_clsses = target_clss.split('|')
#             else:
#                 target_clsses = None
#
#             os.chdir(os.path.join(repo_dir, repo))
#             os.system(f'git checkout {version} >/dev/null 2>&1')
#             os.chdir(cwd)
#             # if not initialized, then calculate it
#             print('='*5, cve, repo, '='*5)
#             top_k = coll.find_one({'cve': cve})['top-k']
#             paths = coll.find_one({'cve': cve})['path']
#             list_to_do=[]
#             ids = []
#             for i, k in enumerate(top_k):
#                 id = k
#                 if k in black_list:
#                     continue
#                 ids.append(id)
#                 if repo_sum.find_one({'id':k}):
#                     continue
#
#
#                 file_path = paths[i]
#                 gav = id.split('@')[0]
#                 file_sig = id+'&&'+file_path + '&&'+gav+'&&'+repo+'&&'+cve
#                 list_to_do.append(file_sig)
#
#             current_doc_num = repo_sum.count_documents({})
#             generate_summaries(list_to_do, current_doc_num)
#             nl_compare(repo_sum, nl_result_coll, ids, repo, cve, description)
#             top_k = nl_result_coll.find_one({'cve': cve})['top-k']
#             tops = list()
#             target_position = 99999
#             for k in top_k:
#                 clss = k.split('@')[-1]
#                 tops.append(clss)
#             if target_clsses:
#                 for t in target_clsses:
#                     if t in tops:
#                         print('[INFO] Found in top', tops.index(t))
#                         correct= True
#                         target_position = min(tops.index(t), target_position)
#                     else:
#                         print('[INFO] Not found', t)
#                 print('\n\n')
#             else:
#                 if target_clss in tops:
#                     correct= True
#                     print('[INFO] Found in top', tops.index(target_clss), '\n\n')
#                     target_position = min(tops.index(target_clss), target_position)
#                 else:
#                     print('[INFO] Not found', target_clss, '\n\n')
#             if correct:
#                 all_cases[0]+=1
#
#                 writer.writerow([cve, repo, correct, target_position, (target_position+1), coll.find_one({'cve':cve})['all_file_count']])
#             else:
#                 writer.writerow([cve, repo, correct, 'None'])
#     c.close()
#     print('Overall correctness', all_cases[0], '/', all_cases[1])


def remove_java_comments(code_string):
    cleaned_code = code_string.replace('/*', '').replace('*/', '').replace('//', '').replace('*', '')
    return cleaned_code


    

def main():
    c = pymongo.MongoClient(mongodb_host, port=mongodb_port)
    sum_coll = c[mongo_db]['repo_summary']
    
    with open('data/dataset - cve data set.csv') as f:
        reader = csv.reader(f)
        next(reader, None)
        for line in reader:
            cve = line[0]
            repo = line[9]
            version = line[11]
            description = line[14]
            sim = pd.DataFrame({}, columns=['class', 'similarity'])
            for doc in sum_coll.find():
                if repo!=doc['repo']:
                    continue
                f = doc['meta']['name']
                sum = ''
                for cls in doc['meta'].get('classes', []):
                    sum = sum + cls['sum'] if 'sum' in cls else cls['comment']
                sum = remove_java_comments(sum)
                sim_value = cosine_sim(sum, description)
                sim = pd.concat([sim, pd.DataFrame({'class': [f], 'similarity': [sim_value]})], ignore_index=True)
            
            sim = sim.sort_values('similarity', ascending=False)
            sim.to_csv(f'data/{cve}_similarity.csv', index=False)
            break
    c.close()




def full_paper_main_function():
    '''
    for function level
    '''
    set_start_method('spawn', force=True)
    tokenizer, model, device = initialize_model(re_rank_approach_choice)
    # To get filtered funcs first
    inference_model_function_parrallel('../maven_repos/', 'results')
    c = pymongo.MongoClient(mongodb_host, port=mongodb_port)
    res_coll = c[mongo_db][final_results_coll]
    start = False
    filtered_funcs = read_filtered_funcs(retrieval_approach_choice)
    if dataset == 'real_world':
        data = 'data/xxx data - real-world.csv'
    else:
        data = 'data/xxx data - filtered_cve_with_link.csv'
    for i, each in enumerate(read_dataset(data)):
    # for i, each in enumerate(read_dataset('data/xxx data - real-world.csv')):
        cve = each['cve']



        if dataset == 'benchmark' and each['same_location'] == '':
            break
        
        # skip unmatched cases for real-world
        if dataset == 'real_world' and each['commit_url'] == '' and retrieval_approach_choice == 'filter_first':
            continue
        if dataset == 'real_world' and each['commit_url'] != '' and retrieval_approach_choice == 'fine_tuned_bert':
            continue
            
        # skip processed
        if res_coll.count_documents({'cve': cve})>0:
            continue
        print(i, '########## Current cve:', cve, final_results_coll)
        description = each['description']
        repo = each['repo']

        repo_path = os.path.join(repo_dir,repo)

        # for no filter, special check
        if retrieval_approach_choice == 'no_filter':
            filtered_funcs_cve = get_all_funcs(repo_path, cve, each)
        else:
            filtered_funcs_cve = filtered_funcs.get(cve, [])

        description, weights = enrich_description(description, c, cve)
        
        if re_rank_approach_choice == 'tf-idf':
            calculate_tf_idf_vectors_func(repo, description, cve, c, filtered_funcs_cve)
        elif re_rank_approach_choice == 'blizzard':
            pass
        elif re_rank_approach_choice == 'bert2' or re_rank_approach_choice == 'bert':
            calculate_bert(description, cve, model, device, c, filtered_funcs_cve)
        elif re_rank_approach_choice == 'kg':
            kg_sim(description, cve, c, filtered_funcs_cve)
        elif re_rank_approach_choice == 'wkg':
            weighted_kg_sim(description, cve, c, filtered_funcs_cve, weights)
        elif re_rank_approach_choice == 'w2v':
            word_2_vec_sim(description, cve, c, filtered_funcs_cve)
        elif re_rank_approach_choice == 'bm25':
            re_rank_bm25(repo, description, cve, c, filtered_funcs_cve)
        elif re_rank_approach_choice == 'none':
            skip_rerank(description, cve, c, filtered_funcs_cve)
    c.close()



def full_paper_main():
    '''
    [Deprecated]for class level
    '''
    set_start_method('spawn', force=True)
    tokenizer, model, device = initialize_model()
    c = pymongo.MongoClient(mongodb_host, port=mongodb_port)
    desc_coll = c[mongo_db]['dissected_description']
    coll = c[mongo_db][tf_idf_coll]
    start = False
    tf_idf_result_file = f'data/results_pre_filter_{query_expansion_choice}.csv'
    if not os.path.exists(tf_idf_result_file):
        with open(tf_idf_result_file, 'w') as f:
            f.write('cve,repo,correct\n')
    tf_idf_processed_list = [coll['cve'] for coll in coll.find()]
    filtered_files = read_filtered_files(query_expansion_choice)
    for i, each in enumerate(read_dataset('data/xxx data - filtered_cve_with_link.csv')):

        cve = each['cve']

        # skip processed
        # if os.path.exists(f'results/{cve}_top_10.csv'):
        #     continue

        # skip processed tf-idf
        if cve in tf_idf_processed_list:    
            continue

        # control starting flag
        # if cve == 'CVE-2015-0886':
        start = True
        if not start:
            continue
        if each['same_location'] == '':
            break
        print(i, '########## Current cve:', cve)
        description = each['description']
        repo = each['repo']
        version = each['version']
        versions = each['versions']
        clazzes = each['clazz']

        clazz = each['clazz']
        if '|' not in clazz:
            clazzes = None
        else:
            clazzes = clazz.split('|')
            
        methods = each['methods']
        output_file_path = 'patches/'+cve+'.patch'
        commit_url = each['commit_url']

        repo_path = os.path.join(repo_dir,repo)
        if not os.path.exists(repo_path) or repo=='jetty.project':
            print('cloning', repo)
            git_clone(repo, each['repo_link'])


        enriched_description = enrich_description(c, cve)
        print('enriched desc:', enriched_description)
        description_tensor = bert_encode(enriched_description, tokenizer, model)


        # tf-idf prefilter

        version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
        checkout_version(repo_path, version, cve)
        calculate_tf_idf_vectors(repo, coll, description, cve, version, filtered_files.get(cve, []))
        correct = False
        top_k = coll.find_one({'cve': cve})['top-k']
        tops = list()

        for k in top_k:
            clss = k.split('@')[-1]
            tops.append(clss)
        if clazzes:
            for t in clazzes:
                if t in tops:
                    print('[INFO] Found in top', tops.index(t))
                    correct= True
                else:
                    print('[INFO] Not found', t)
            print('\n\n')
        else:
            if clazz in tops:
                correct= True
                print('[INFO] Found in top', tops.index(clazz), '\n\n')
            else:
                print('[INFO] Not found', clazz, '\n\n')
        with open(tf_idf_result_file, 'a') as fw:
            writer = csv.writer(fw)
            writer.writerow([cve, repo, correct])
        

        # debug for tf-idf    
        # continue


        # Read file features from mongodb
        files_properties = read_mongodb_file_properties(repo, cve, versions, c)
        if len(files_properties)==0:
            # preprocess
            version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
            checkout_version(repo_path, version, cve)
            encode_files(repo, cve, version, tokenizer, model, device, c)
            files_properties = read_mongodb_file_properties(repo, cve, versions, c)
        print(len(files_properties), 'files are compared.')
        for file in tqdm(files_properties):
            if 'meta' not in file:
                continue
            if 'classes' not in file['meta']:
                continue
            if len(file['meta']['classes'])==0:
                continue
            if 'sum' in file['meta']['classes'][0]:
                file_tensor = convert_to_tensor(file['meta']['classes'][0]['sum']) #convert_to_tensor(file['meta']['classes'][0]['sum'])
            elif 'body' in file['meta']['classes'][0]:
                file_tensor = bert_encode(file['meta']['classes'][0]['body'], tokenizer, model)
            else:
                file['sim'] = 0
                continue
            file['sim'] = torch.nn.functional.cosine_similarity(description_tensor, file_tensor).item()
        
        df = pd.DataFrame(files_properties)
        df = df.drop('meta', axis=1)
        df = df.sort_values('sim', ascending=False)
        # get top 10
        top_10 = df.head(10)
        # save to csv
        os.makedirs('results', exist_ok=True)
        top_10.to_csv(f'results/{cve}_top_10.csv', index=False)
        # print(clazzes, methods)
    c.close()
