import csv
import json
import os
import random
import pandas as pd
import torch
import concurrent.futures

from tqdm import tqdm
from config import repo_dir
from torch.utils.data import DataLoader, random_split
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, load_metric
from utils.filter_files_basedon_patch import extract_funcs_from_file
from utils.miscellaneous import checkout_version, get_vulnerable_commit, git_clone
from utils.parse_java import parse_java

def read_dataset(dataset):
    ret = []
    if dataset == 'data/xxx data - cve with link.csv' or dataset== 'data/xxx data - filtered_cve_with_link.csv':
        with open(dataset) as f:
            reader = csv.reader(f)
            next(reader, None)
            for i, line in enumerate(reader):
                ret.append({'cve': line[0],
                            'commit_url': line[2],
                            'repo': line[2].split('commit/')[0].split('/')[-2],
                            'version': line[7].split('|')[-1],
                            'versions':line[7].split('|'),
                            'description': line[3],
                            'clazz': line[8],
                            'methods': line[9],
                            'repo_link': line[2].split('commit/')[0],
                            'same_location': line[14]
                            })
    elif dataset=='xxx data - selected.csv':
        with open(dataset) as f:
            reader = csv.reader(f)
            next(reader, None)
            for i, line in enumerate(reader):
                ret.append({'cve': line[0],
                            'repo': line[9],
                            'version': line[11].split('|')[-1],
                            'versions':line[11].split('|'),
                            'description': line[14],
                            'clazz': line[3],
                            'methods': line[4],
                            'repo_link': line[1]
                            })
    return ret


def read_positive_files():
    with open('with_patch_results/target_funcs_from_patch.json', 'r') as f:
        targets = json.load(f)
    with open('with_patch_results/original_patch_funcs.json', 'r') as f:
        original = json.load(f)
    ret = {}
    dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
    for each in dataset:
        cve = each['cve']
        ret[cve] = []
        if cve not in targets:
            continue
        for list_of_funcs in targets[cve]:
            ret[cve].extend(list_of_funcs)
        ret[cve].extend(original[cve])
        # if len(ret[cve]) == 0:
        #     print('no files for:', cve) 
    return ret


def get_all_files(repo):
    all_files = []
    if 'maven_repos' not in repo:
        target_repo_path = os.path.join(repo_dir, repo)
    else:
        target_repo_path = repo
    for root, dir, fs in os.walk(target_repo_path):
        for f in fs:
            if not f.endswith('.java') or 'test' in root or 'Test' in root or '/mock/' in root:
                continue
            file_path = os.path.join(root, f)
            all_files.append(file_path)
    return all_files


def get_all_funcs(repo_path, cve, data):
    if os.path.exists('funcs/'+cve+'/saved_funcs.json'):
        os.makedirs('funcs/'+cve+'/', exist_ok=True)
        with open('funcs/'+cve+'/saved_funcs.json') as f:
            return json.load(f)
    repo = data['repo']
    if not os.path.exists(repo_path):
        print('cloning', repo)
        git_clone(repo, data['repo_link'])
    if not os.path.exists(repo_path):
        return []
    version = data['version']
    output_file_path = 'patches/'+cve+'.patch'
    commit_url = data['commit_url']
    version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
    checkout_version(repo_path, version, cve)
    files = get_all_files(repo_path)


    results = []
    # for each in negative_files:
    #     fs = extract_funcs_from_file(each)
    #     if len(fs)>0:
    #         results[cve].append(random.choice(fs))
    print('list to do', len(files))
    with concurrent.futures.ProcessPoolExecutor(20) as executor:
        # Using a list to store futures
        futures = []
        
        for item in files:
            # Submitting the function to be executed with fixed arguments and an element from the list
            future = executor.submit(extract_funcs_from_file, item)
            futures.append(future)
        
        # Gathering results (if needed)
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc="Processing Files"):
            try:
                result = future.result()
                if len(result)>0:
                    results.extend(result)
            except Exception as exc:
                print(f"Generated an exception: {exc}")
    os.makedirs('funcs/'+cve+'/', exist_ok=True)
    with open('funcs/'+cve+'/saved_funcs.json', 'w') as fw:
        fw.write(json.dumps(results))
    return results

def save_negative_data(rate:int=100):
    target_funcs = read_positive_files()
    # print(len(target_funcs.keys()))
    if os.path.exists('with_patch_results/negative_files.json'):
        results = json.load(open('with_patch_results/negative_files.json', 'r'))
    else:
        results = {}
    for i, data in enumerate(read_dataset('data/xxx data - filtered_cve_with_link.csv')):
        cve = data['cve']
        if cve not in target_funcs:
            continue
        if cve in results:
            continue
        print(i, '########## Current cve:', cve)
        methods = target_funcs[cve]
        if len(methods)==0:
            print('no funcs associated')
            continue
        target_files = [each.get('file', '') for each in methods]
        repo = data['repo']
        repo_path = os.path.join(repo_dir,repo)
        version = data['version']
        if data['same_location'] != '':
            continue
        if not os.path.exists(repo_path):
            print('cloning', repo)
            git_clone(repo, data['repo_link'])

        if not os.path.exists(repo_path):
            continue
        
        output_file_path = 'patches/'+cve+'.patch'
        commit_url = data['commit_url']
        version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
        checkout_version(repo_path, version, cve)

        files = get_all_files(repo)

        negative_files = list(set(files) - set(target_files))
        random.shuffle(negative_files)
        negative_files = negative_files[:len(target_files)*rate]
        results[cve] = []
        # for each in negative_files:
        #     fs = extract_funcs_from_file(each)
        #     if len(fs)>0:
        #         results[cve].append(random.choice(fs))
        print('list to do', len(negative_files))
        with concurrent.futures.ProcessPoolExecutor() as executor:
            # Using a list to store futures
            futures = []
            
            for item in negative_files:
                # Submitting the function to be executed with fixed arguments and an element from the list
                future = executor.submit(extract_funcs_from_file, item)
                futures.append(future)
            
            # Gathering results (if needed)
            for future in concurrent.futures.as_completed(futures):
                try:
                    result = future.result()
                    if len(result)>0:
                        results[cve].extend(result)
                except Exception as exc:
                    print(f"Generated an exception: {exc}")
        # print('Neg files length:', len(negative_files), negative_files[0])
        with open('with_patch_results/negative_files.json', 'w') as f:
            json.dump(results, f)

def read_negative_files():
    with open('with_patch_results/negative_files.json', 'r') as f:
        return json.load(f)

def prepare_data():
    os.makedirs('fine_tune', exist_ok=True)
    if not os.path.exists('fine_tune/prepare_data.json'):
        print('preparing data')
        positive_files = read_positive_files()
        negative_files = read_negative_files()
        print('positive files:', len(positive_files.keys()))
        print('negative files:', len(negative_files.keys()))
        dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
        all_data = {}
        all_data['description'] = []
        all_data['file_content'] = []
        all_data['labels'] = []
        for i, data in enumerate(dataset):
            cve = data['cve']
            if i % 100 == 0:
                print(i, '########## Current cve:', cve)
            repo = data['repo']
            repo_path = os.path.join(repo_dir,repo)
            if cve in positive_files:
                for func in positive_files[cve]:
                    if func['comment']:
                        all_data['file_content'].append(func['comment']+'\n'+func['body'])
                    else:
                        all_data['file_content'].append(func['body'])
                    all_data['description'].append(data['description'])
                    all_data['labels'].append(1)
            if cve in negative_files:
                for func in negative_files[cve]:
                    if func['comment']:
                        all_data['file_content'].append(func['comment']+'\n'+func['body'])
                    else:
                        all_data['file_content'].append(func['body'])
                    all_data['description'].append(data['description'])
                    all_data['labels'].append(0)
            print('All data length:', len(all_data['description']))
        with open('fine_tune/prepare_data.json', 'w') as f:
            json.dump(all_data, f)
    else:
        with open('fine_tune/prepare_data.json', 'r') as f:
            all_data = json.load(f)
    return all_data



    # train_data = {
    #     "description": ["CVE description 1", "CVE description 2"],
    #     "file_content": ["java code snippet 1", "java code snippet 2"],
    #     "labels": [1, 0]  # 1 for relevant, 0 for not relevant
    # }
    # val_data = {
    #     "description": ["CVE description 3", "CVE description 4"],
    #     "file_content": ["java code snippet 3", "java code snippet 4"],
    #     "labels": [1, 0]
    # }

def fine_tune():
    import os
    # os.environ["CUDA_VISIBLE_DEVICES"] = "7"
    # Check the total number of GPUs available
    num_gpus = torch.cuda.device_count()
    print(f"Total GPUs available: {num_gpus}")
    if not os.path.exists('fine_tune/train_dataset') and not os.path.exists('fine_tune/val_dataset'):

        all_data = prepare_data()
        # Truncate all_data to first 3k
        for key in all_data:
            all_data[key] = all_data[key]
        # Split the data into training and validation sets with 8:2
        all_indexes = list(range(len(all_data['description'])))
        train_size = int(0.8 * len(all_indexes))
        val_size = len(all_indexes) - train_size
        train_indexes, val_indexes = random_split(all_indexes, [train_size, val_size])
        train_data = {}
        val_data = {}
        for key in all_data:
            train_data[key] = [all_data[key][i] for i in train_indexes]
            val_data[key] = [all_data[key][i] for i in val_indexes]
        



        train_df = pd.DataFrame(train_data)
        val_df = pd.DataFrame(val_data)

        def tokenize_function(examples):
            # Concatenate description and file_content with a separator for each example in the batch
            texts = [desc + tokenizer.sep_token + content for desc, content in zip(examples["description"], examples["file_content"])]
            # Tokenize all concatenated pairs
            return tokenizer(texts, padding="max_length", truncation=True, return_tensors="np")
        tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')

        # Convert DataFrames to Hugging Face Dataset
        train_dataset = Dataset.from_pandas(train_df)
        val_dataset = Dataset.from_pandas(val_df)

        # Apply the tokenizer to the datasets
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['description', 'file_content'])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['description', 'file_content'])

        train_dataset = train_dataset.rename_column("labels", "label")
        val_dataset = val_dataset.rename_column("labels", "label")

        # Set format for PyTorch
        # train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
        # val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

        train_dataset.save_to_disk('fine_tune/train_dataset')
        val_dataset.save_to_disk('fine_tune/val_dataset')
    else:
        train_dataset = Dataset.load_from_disk('fine_tune/train_dataset')
        val_dataset = Dataset.load_from_disk('fine_tune/val_dataset')

    
    model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)
    # Force the device to be CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device = torch.device("cpu")
    print('Using', device)

    if device.type == 'cuda':
        torch.cuda.set_device(0) # Set the CUDA device, if multiple devices are available
        total_memory = torch.cuda.get_device_properties(0).total_memory
        available_memory = total_memory - torch.cuda.memory_allocated(0)
        print(f"Total Memory: {total_memory / (1024**3):.2f} GB")
        print(f"Available Memory: {available_memory / (1024**3):.2f} GB")
    else:
        print("CUDA is not available. Check your installation or if your device supports CUDA.")
    model.to(device)  # Move the model to the CPU
    training_args = TrainingArguments(
        output_dir='./my_model',
        num_train_epochs=3,
        per_device_train_batch_size=42,
        # gradient_accumulation_steps=4,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        evaluation_strategy="epoch",  # Change to evaluate at the end of each epoch
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    trainer.train()
    os.makedirs("./my_model/", exist_ok=True)
    model_save_path = "./my_model/"
    trainer.save_model(model_save_path)
    evaluation_results = trainer.evaluate()
    print(evaluation_results)