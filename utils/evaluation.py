import concurrent.futures
import csv
import json
from multiprocessing import set_start_method
import os
import pymongo
from sklearn.metrics import precision_score, recall_score, f1_score
import torch
from config import mongodb_host, mongodb_port, repo_dir, final_results_coll, dataset, retrieval_approach_choice
from main.fine_tune_function import get_all_files, get_all_funcs
from main.inference_model import inference_model, inference_model_function, load_model
from utils.enrich_description import query_expansion_one
from utils.repo_file_utils import read_filtered_files, read_filtered_funcs, read_relevant_files
from main.fine_tune import read_dataset
from tqdm import tqdm
from utils.miscellaneous import checkout_version, get_func_id, get_vulnerable_commit, git_clone, read_mongodb_file_properties, get_func_hash, get_func_id
from utils.parse_java import parse_java
from utils.repo_file_utils import read_relevant_files

def eval_tf_idf_util(cve, coll, clazzes):
    correct = False
    top_k = coll.find_one({'cve': cve})['top-k']
    tops = list()
    rank = None

    for k in top_k:
        if '@' in k:
            clss = k.split('@')[-1]
        else:
            clss = k
        tops.append(clss)

    for t in clazzes:
        if t in tops:
            print('[INFO] Found in top', tops.index(t))
            correct= True
            if rank is None or tops.index(t) < rank:
                rank = tops.index(t)

        else:
            print('[INFO] Not found', t)
    # print('\n\n')

    return correct, rank

def calculate_average_precision(predictions, ground_truth):
    # print('Prediction:', predictions)
    # print('Ground truth:', ground_truth)
    # Initialize counters
    true_positives = 0
    false_positives = 0
    false_negatives = len(ground_truth)
    
    # Initialize list to store precision at each relevant document
    precisions = []
    
    # Loop through each prediction
    for i, prediction in enumerate(predictions):
        if prediction in ground_truth:
            true_positives += 1
            false_negatives -= 1
            precision = true_positives / (true_positives + false_positives)
            precisions.append(precision)
        else:
            false_positives += 1
    
    # Calculate average precision
    if precisions:
        average_precision = sum(precisions) / len(ground_truth)
    else:
        average_precision = 0
    
    return average_precision

def evaluation():

    # Initialize a list to store all AP values
    # Initialize variables to store AP values and reciprocal ranks
    ap_values = []
    reciprocal_ranks = []

    with open('data/xxx data - filtered_cve_with_link.csv') as f:
        for line in csv.reader(f):
            # Get the target class
            target_class = get_ground_truth_classes(line)
            cve = line[0]

            # if cve!='CVE-2018-1000850':
            #     continue

            # stop after manual inspection results when if location same is null
            if len(line[14])==0:
                break

            if os.path.exists(f'results/{cve}_top_10.csv'):
                # read column class with pandas
                import pandas as pd
                df = pd.read_csv(f'results/{cve}_top_10.csv')
                predictions = list(df['class'].tolist())

                ap = calculate_average_precision(predictions, target_class)
                
                # Find the rank of the first relevant file
                for rank, prediction in enumerate(predictions, start=1):
                    if prediction in target_class:
                        reciprocal_ranks.append(1 / rank)
                        break
                else:  # No relevant file found
                    reciprocal_ranks.append(0)  # Or treat according to your handling of such cases
                
                print(f'AP for {cve}: {ap:.2f}')
                ap_values.append(ap)

    # Calculate mAP if there are any AP values
    if ap_values:
        mAP = sum(ap_values) / len(ap_values)
        print(f'mAP: {mAP:.2f}')
    else:
        print('No AP values to calculate mAP.')

    # Calculate MRR
    if reciprocal_ranks:
        MRR = sum(reciprocal_ranks) / len(reciprocal_ranks)
        print(f'MRR: {MRR:.2f}')
    else:
        print('No Reciprocal Ranks to calculate MRR.')


def get_ground_truth_classes(line):
    target_class = []#line[8].split('|')
    if line[16] != '':
        target_class.extend(line[16].split('|'))
    else: 
        target_class  = line[8].split('|')
    target_class = [each.strip() for each in target_class]
    target_class = list(set(target_class))
    return target_class

def get_ground_truth_funcs(line):
    target_funcs = []#line[8].split('|')
    if line[15] != '':
        target_funcs.extend(line[15].split('|'))
    else: 
        target_funcs  = [each.split(':')[-1] for each in line[9].split('|')]
    target_funcs = [each.strip() for each in target_funcs]
    target_funcs = list(set(target_funcs))
    return target_funcs

def get_ground_truth_from_cve(cve):
    with open('data/xxx data - filtered_cve_with_link.csv') as f:
        for line in csv.reader(f):
            if line[0] == cve:
                return get_ground_truth_classes(line)

def get_gt_funcs_from_cve(cve):
    with open('data/xxx data - filtered_cve_with_link.csv') as f:
        for line in csv.reader(f):
            if line[0] == cve:
                return get_ground_truth_funcs(line)
    return []

def evaluate_re_rank():
    in_tf_idf_cve_rank = {}
    ap_values = []
    reciprocal_ranks = []
    total_cve = set()
    found_count =0
    c = pymongo.MongoClient(mongodb_host, mongodb_port)
    coll = c['maven_vul_mapping_xxx_data'][tf_idf_coll]
    
    for data in read_dataset('data/xxx data - filtered_cve_with_link.csv'):
        cve = data['cve']
        if cve == 'cve':
            continue
        
        # if cve!='CVE-2020-1953':
        #     continue
        
        # if len(data[''])==0:
            # break
        if data['same_location'] == '':
            break
        total_cve.add(cve)
        print('########## Current cve:', cve)

        target_class = get_ground_truth_from_cve(cve)
        all_predictions = [each.split('@')[-1] for each in coll.find_one({'cve': cve})['top-k']][:10]
        found, rank = eval_tf_idf_util(cve, coll, target_class) 
        if found:
            found_count += 1
            in_tf_idf_cve_rank[cve] = rank
        predictions = all_predictions

        ap = calculate_average_precision(predictions, target_class)
        # Find the rank of the first relevant file
        for rank, prediction in enumerate(predictions, start=1):
            if prediction in target_class:
                reciprocal_ranks.append(1 / rank)
                break
        else:  # No relevant file found
            reciprocal_ranks.append(0)  # Or treat according to your handling of such cases
        if data['same_location'] == 'no' and reciprocal_ranks[-1]==0:
            print('[INFO] Not same location')
            print(data['commit_url'])
        print('Reciprocal rank:', reciprocal_ranks[-1])
        print(f'AP for {cve}: {ap:.2f}')
        ap_values.append(ap)

    # Calculate mAP if there are any AP values
    if ap_values:
        mAP = sum(ap_values) / len(ap_values)
        print(f'mAP: {mAP:.2f}')
    else:
        print('No AP values to calculate mAP.')

    # Calculate MRR
    if reciprocal_ranks:
        MRR = sum(reciprocal_ranks) / len(reciprocal_ranks)
        print(f'MRR: {MRR:.2f}')
    else:
        print('No Reciprocal Ranks to calculate MRR.')
    print(sum(in_tf_idf_cve_rank.values())/len(in_tf_idf_cve_rank.keys()), 'average rank in tf-idf')

    print('Found', found_count, 'cves in tf-idf')
    print('Total cve:', len(total_cve))

def evaluate_re_rank_func():
    in_tf_idf_cve_rank = {}
    ap_values = []
    reciprocal_ranks = []
    total_cve = set()
    found_count =0
    if os.path.exists('data/results_summary.json'):
        results_summary = json.load(open('data/results_summary.json'))
    else:
        results_summary = {}
    recall_at_k_values = {k: [] for k in range(1, 11)}  # Initialize recall values for K=1 to 10
    manual_efforts = {k: [] for k in list(range(1, 11))+[20, 30, 50, 100]}
    c = pymongo.MongoClient(mongodb_host, mongodb_port)
    coll = c['maven_vul_mapping_xxx_data'][final_results_coll]
    print(final_results_coll)
    if dataset == 'real_world':
        data = 'data/xxx data - real-world.csv'
    else:
        data = 'data/xxx data - filtered_cve_with_link.csv'
    with open('data/small_rr.csv', 'w') as f:
        csv.writer(f).writerow(['cve', 'rr', 'method1', 'class1', 'method2', 'class2', 'method3', 'class3', 'method4', 'class4', 'method5', 'class5', 'method6', 'class6', 'method7', 'class7', 'method8', 'class8', 'method9', 'class9', 'method10', 'class10'])
    for data in read_dataset(data):
        cve = data['cve']
        if cve == 'cve':
            continue
        
        # if cve!='CVE-2014-0054':
        #     continue
        
        # if len(data[''])==0:
            # break
        if dataset == 'benchmark' and  data['same_location'] == '':
            break

        # skip unmatched cases for real-world
        if dataset == 'real_world' and data['commit_url'] == '' and retrieval_approach_choice == 'filter_first':
            continue
        if dataset == 'real_world' and data['commit_url'] != '' and retrieval_approach_choice == 'fine_tuned_bert':
            continue
        # if data['same_location'] == 'no':
            # print('[INFO] Not same location') 
            # print(data['commit_url'])
            # continue
        total_cve.add(cve)
        print('########## Current cve:', cve, final_results_coll)

        target_funcs = get_gt_funcs_from_cve(cve)
        predictions = [each for each in coll.find_one({'cve': cve})['top-k']]
        if coll.find_one({'cve': cve}).get('path', []) != []:
            classes = [each.split('/')[-1].replace('.java', '') for each in coll.find_one({'cve': cve})['path']]
        else:
            classes = ['' for each in predictions]
        prediction_classes = {}
        for i, prediction in enumerate(predictions):
            prediction_classes[prediction] = classes[i]

        # deduplicate predictions while keep the order
        seen = set()
        predictions = [x for x in predictions if not (x in seen or seen.add(x))]
        seen = set()
        classes = [x for x in classes if not (x in seen or seen.add(x))]



        # print('[Predictions]', predictions)
        # print('[Ground truth]', target_funcs)
        ap = calculate_average_precision(predictions, target_funcs)
        # Find the rank of the first relevant file
        for rank, prediction in enumerate(predictions, start=1):
            if prediction in target_funcs:
                reciprocal_ranks.append(1 / rank)
                break
        else:  # No relevant file found
            reciprocal_ranks.append(0)  # Or treat according to your handling of such cases

        print('Reciprocal rank:', reciprocal_ranks[-1])
        write_content = [cve, reciprocal_ranks[-1]]
        for i, prediction in enumerate(predictions):
            if i == 10:
                break
            write_content.append(prediction)
            write_content.append(prediction_classes[prediction])

        with open('data/small_rr.csv', 'a') as f:
            csv.writer(f).writerow(write_content)

        print(f'AP for {cve}: {ap:.2f}')
        ap_values.append(ap)

        # Add recall@K calculation
        for k in range(1, 11):
            # Calculate the number of target functions found in the top K predictions
            # found_in_top_k = sum(1 for func in predictions[:k] if func in target_funcs)
            recall_at_k = 0#found_in_top_k / len(target_funcs) if target_funcs else 0
            for func in predictions[:k]:
                if func in target_funcs:
                    recall_at_k = 1
            recall_at_k_values[k].append(recall_at_k)

        # Calculate Manual Effort for K=1 to 100
        for k in manual_efforts.keys():
            effort = k  # Default to 1 if no relevant items are found within the top K
            for rank, prediction in enumerate(predictions[:k], start=1):
                if prediction in target_funcs:
                    effort = rank 
                    break  # Stop after finding the first relevant item within the top K
            manual_efforts[k].append(effort)
    # Calculate mAP if there are any AP values
    if ap_values:
        mAP = sum(ap_values) / len(ap_values)
        print(f'mAP: {mAP:.2f}')
    else:
        print('No AP values to calculate mAP.')

    # Calculate MRR
    if reciprocal_ranks:
        MRR = sum(reciprocal_ranks) / len(reciprocal_ranks)
        print(f'MRR: {MRR:.5f}')
    else:
        MRR = 0
        mAP = 0
        print('No Reciprocal Ranks to calculate MRR.')
    # print(sum(in_tf_idf_cve_rank.values())/len(in_tf_idf_cve_rank.keys()), 'average rank in tf-idf')
    results_summary[final_results_coll] = {'mRR': MRR, 'mAP': mAP}
    # After processing all CVEs, calculate the average recall@K over all CVEs
    print("\nFinal Recall@K values:")
    for k in range(1, 11):
        results_summary[final_results_coll][f'recall@{k}'] = sum(recall_at_k_values[k]) / len(recall_at_k_values[k]) if recall_at_k_values[k] else 0
        avg_recall_at_k = sum(recall_at_k_values[k]) / len(recall_at_k_values[k]) if recall_at_k_values[k] else 0
        print(f'Average Recall@{k}: {avg_recall_at_k:.5f}')
    print('Found', found_count, 'cves in tf-idf')
    print('Total cve:', len(total_cve))

    for k in manual_efforts.keys():
        average_effort = sum(manual_efforts[k]) / len(manual_efforts[k])
        print(f'Manual Efforst@{k}: {average_effort:.2f}')
        results_summary[final_results_coll][f'manual_effort@{k}'] = average_effort


    # convert results_summary to csv
    with open('data/results_summary.json', 'w') as f:
        json.dump(results_summary, f)

    with open('data/results_summary.csv', 'w') as f:
        csv.writer(f).writerow(['qe', 'retrieval', 'rerank', 'mRR', 'mAP', 'recall@1', 'recall@2', 'recall@3', 'recall@4', 'recall@5', 'recall@6', 'recall@7', 'recall@8', 'recall@9', 'recall@10', 'ME@1', 'ME@2', 'ME@3', 'ME@4', 'ME@5', 'ME@6', 'ME@7', 'ME@8', 'ME@9', 'ME@10', 'ME@20', 'ME@30', 'ME@50', 'ME@100'])
        for model, results in results_summary.items():
            splitted = model.split('__')
            if len(splitted) == 3:
                retrieval, qe, rerank = splitted
            elif len(splitted) == 4:
                retrieval, qe,  rerank, dataset_ = splitted
            csv.writer(f).writerow([qe, retrieval, rerank, results['mRR'], results['mAP'], results[f'recall@1'], results[f'recall@2'], results[f'recall@3'], results[f'recall@4'], results[f'recall@5'], results[f'recall@6'], results[f'recall@7'], results[f'recall@8'], results[f'recall@9'], results[f'recall@10'], results[f'manual_effort@1'], results[f'manual_effort@2'], results[f'manual_effort@3'], results[f'manual_effort@4'], results[f'manual_effort@5'], results[f'manual_effort@6'], results[f'manual_effort@7'], results[f'manual_effort@8'], results[f'manual_effort@9'], results[f'manual_effort@10'], results[f'manual_effort@20'], results[f'manual_effort@30'], results[f'manual_effort@50'], results[f'manual_effort@100']])


def eval_model():
    set_start_method('spawn', force=True)
    model, tokenizer, device = load_model('my_model')
    worker_num = 4
    ground_truth_files = read_filtered_files()
    result_file = 'evaluation_results_function_1-100.json'
    dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
    if os.path.exists(result_file):

        results = json.load(open(result_file))
    else:
        results = {}


    for i, data in enumerate(dataset):
        
        cve = data['cve']
        repo = data['repo']
        repo_path = os.path.join(repo_dir,repo)           
        cve_description = data['description']
        methods = data['methods']
        version = data['version']
        output_file_path = 'patches/'+cve+'.patch'
        commit_url = data['commit_url']
        if data['same_location'] == '':
            break
        print(i, '########## Current cve:', cve)
        if not os.path.exists(repo_path):
            print('cloning', repo)
            git_clone(repo, data['repo_link'])
        version = get_vulnerable_commit(output_file_path, commit_url, repo_path, version)
        checkout_version(repo_path, version, cve)
        all_files = read_relevant_files(repo_path)
        gt_files_4_cve = [each_file.split('/maven_repos/')[-1] for each_file in ground_truth_files[cve]]
        if cve in results:
            predicted_files = results[cve][3]
        else:
            
            list_to_do = [(sentence, tokenizer, model, device, cve_description, index, len(all_files)) for index, sentence in enumerate(all_files)]

            predicted_files = []
            # for item in tqdm(list_to_do):
            #     result = inference_model(item)
            #     if result:
            #         predicted_files.append(result)
            with concurrent.futures.ProcessPoolExecutor(64) as executor:
                # Using a list to store futures
                futures = []
                
                for item in list_to_do:
                    # Submitting the function to be executed with fixed arguments and an element from the list
                    future = executor.submit(inference_model, item)
                    futures.append(future)
                
                # Gathering results (if needed)
                for future in concurrent.futures.as_completed(futures):
                    try:
                        result = future.result()
                        if result:
                            predicted_files.append(result.split('/maven_repos/')[-1]) 
                    except Exception as exc:
                        print(f"Generated an exception: {exc}")


        
        # Convert lists/sets to binary labels: 1 if relevant, 0 otherwise
        # Assuming 'all_files' is a list of all possible files
        y_true = [1 if file.split('/maven_repos/')[-1] in gt_files_4_cve else 0 for file in all_files]
        y_pred = [1 if file.split('/maven_repos/')[-1] in predicted_files else 0 for file in all_files]

        precision = precision_score(y_true, y_pred)
        recall = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)
        print(predicted_files)
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1 Score: {f1:.4f}")
        print('Done', cve)
        results[cve] =[precision, recall, f1, predicted_files, gt_files_4_cve]
        with open(result_file, 'w') as f:
            json.dump(results, f)


def parrallel_one(args):
    i, data, tokenizer, model, device, ground_truth_funcs, result_file = args
    cve = data['cve']
    repo = data['repo']
    repo_path = os.path.join(repo_dir,repo)           
    cve_description = data['description']
    print(i, '########## Current cve:', cve)
    all_funcs = get_all_funcs(repo_path, cve, data)
    gt_funcs_4_cve = [get_func_id(each_func) for each_func in ground_truth_funcs]
    print('funcs to do', len(all_funcs))

    list_to_do = [(func, tokenizer, model, device, cve_description, index, len(all_funcs)) for index, func in enumerate(all_funcs)]

    predicted_funcs = {}
    probs = []
    for item in list_to_do:
        func, prob  = inference_model_function(item)
        if func:
            probs.append(prob)


            predicted_funcs[get_func_hash(func)] = {'score': prob, 'func': func}
    # Sort the predicted functions by score
    predicted_funcs = sorted(predicted_funcs.items(), key=lambda x: x[1]['score'], reverse=True)

    funcs = [each[1]['func'] for each in predicted_funcs][:300]
    scores = [each[1]['score'] for each in predicted_funcs][:300]
    # Convert lists/sets to binary labels: 1 if relevant, 0 otherwise
    # Assuming 'all_files' is a list of all possible files
    predicted_func_ids = []
    for i, func in enumerate(funcs[:100]):
        predicted_func_ids.append(get_func_id(func))
    y_true = [1 if get_func_id(func) in gt_funcs_4_cve else 0 for func in all_funcs]
    y_pred = [1 if get_func_id(func) in predicted_func_ids else 0 for func in all_funcs]

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    print('CVE:', cve)
    # print('number of predicted', len(predicted_funcs))
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print('Done', cve)
    # store precision, recall, f1, predicted_func_ids, gt_funcs_4_cve, funcs, scores
    results ={
        'precision': precision, 
        'recall': recall, 
        'f1': f1,
        'predicted_func_ids': predicted_func_ids,
        'gt_funcs_4_cve': gt_funcs_4_cve,
        'funcs': funcs,
        'scores': scores
        }
    with open(result_file+'/top000_'+cve+'.json', 'w') as fw:
        fw.write(json.dumps(results))

def eval_model_function_parrallel(result_file, results_folder, start_number, stop_number):
    os.makedirs(results_folder, exist_ok=True)
    set_start_method('spawn', force=True)
    model, tokenizer, device = load_model('model_12_03_1-10')
    ground_truth_funcs = read_filtered_funcs()
    dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
    list_to_do = []
    for i, data in enumerate(dataset):
        if i>=start_number and i<stop_number:
            cve = data['cve']
            if os.path.exists(results_folder+'/top000_'+cve+'.json'):
                continue
            # if cve in results:
                # with open(results_folder+'/top000_'+cve+'.json', 'w') as fw:
                #     fw.write(json.dumps(results[cve]))
                # continue
            if data['same_location'] == '':
                continue
            list_to_do.append((i, data, tokenizer, model, device, ground_truth_funcs[cve], results_folder))
    print('list to do', len(list_to_do))
    with concurrent.futures.ProcessPoolExecutor(5) as executor:
        # Using a list to store futures
        futures = []
        
        for item in list_to_do:
            # Submitting the function to be executed with fixed arguments and an element from the list
            future = executor.submit(parrallel_one, item)
            futures.append(future)
        
        # Gathering results (if needed)
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Inference model'):
            try:
                _ = future.result()
            except Exception as exc:
                print(f"Generated an exception: {exc}")

def inference_model_function_parrallel(result_file, results_folder):
    os.makedirs(results_folder, exist_ok=True)
    set_start_method('spawn', force=True)
    model, tokenizer, device = load_model('model_12_03_1-10')
    ground_truth_funcs = read_filtered_funcs()
    dataset = read_dataset('data/xxx data - real-world.csv')
    list_to_do = []
    for i, data in enumerate(dataset):
        cve = data['cve']
        if os.path.exists(results_folder+'/top000_'+cve+'.json'):
            continue
        # if cve in results:
            # with open(results_folder+'/top000_'+cve+'.json', 'w') as fw:
            #     fw.write(json.dumps(results[cve]))
            # continue
        list_to_do.append((i, data, tokenizer, model, device, [], results_folder))
    print('list to do', len(list_to_do))
    for each in list_to_do:
        parrallel_one(each)
    exit()
    with concurrent.futures.ProcessPoolExecutor(5) as executor:
        # Using a list to store futures
        futures = []
        
        for item in list_to_do:
            # Submitting the function to be executed with fixed arguments and an element from the list
            future = executor.submit(parrallel_one, item)
            futures.append(future)
        
        # Gathering results (if needed)
        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Inference model'):
            try:
                _ = future.result()
            except Exception as exc:
                print(f"Generated an exception: {exc}")


def eval_model_function(repo_dir, result_file, start_number, stop_number):
    print('Current repo director:', repo_dir)
    set_start_method('spawn', force=True)
    model, tokenizer, device = load_model('model_18_03_1-100')
    worker_num = 4
    ground_truth_funcs = read_filtered_funcs()
    dataset = read_dataset('data/xxx data - filtered_cve_with_link.csv')
    if os.path.exists(result_file):

        results = json.load(open(result_file))
    else:
        results = {}
    for i, data in enumerate(dataset):
        if i>=start_number and i<stop_number:
            cve = data['cve']
            repo = data['repo']
            repo_path = os.path.join(repo_dir,repo)           
            cve_description = data['description']

            # if data['same_location'] == '':
            #     break
            print(i, '########## Current cve:', cve)
            if cve in results:
                print('Already proccessed')
                continue
            
            all_funcs = get_all_funcs(repo_path, cve, data)
            
            




            gt_funcs_4_cve = [get_func_id(each_func) for each_func in ground_truth_funcs[cve]]
            print('funcs to do', len(all_funcs))
            if cve in results:
                predicted_funcs = results[cve][3]
            else:
                list_to_do = [(func, tokenizer, model, device, cve_description, index, len(all_funcs)) for index, func in enumerate(all_funcs)]
            predicted_funcs = {}
            for item in tqdm(list_to_do):
                func, prob  = inference_model_function(item)
            #     if func:
            # with concurrent.futures.ProcessPoolExecutor(60) as executor:

            #     futures = []
            #         predicted_funcs[get_func_hash(func)] = {'score': prob, 'func': func}
            #     for item in list_to_do:
            # predicted_funcs = sorted(predicted_funcs.items(), key=lambda x: x[1]['score'], reverse=True)
            #         future = executor.submit(inference_model_function, item)
            #         futures.append(future)
                
            #     # Gathering results (if needed)
            #     for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Inference model'):
            #         try:
            #             result = future.result()
            #             if result:
            #                 predicted_funcs.append(get_func_id(result)) 
            #         except Exception as exc:
            #             print(f"Generated an exception: {exc}")
            predicted_funcs[get_func_hash(func)] = {'score': prob, 'func': func}
            # Sort the predicted functions by score
            predicted_funcs = sorted(predicted_funcs.items(), key=lambda x: x[1]['score'], reverse=True)

            funcs = [each[1]['func'] for each in predicted_funcs][:300]
            scores = [each[1]['score'] for each in predicted_funcs][:300]
            # Convert lists/sets to binary labels: 1 if relevant, 0 otherwise
            # Assuming 'all_files' is a list of all possible files
            predicted_func_ids = []
            for i, func in enumerate(funcs[:100]):
                predicted_func_ids.append(get_func_id(func))
            y_true = [1 if get_func_id(func) in gt_funcs_4_cve else 0 for func in all_funcs]
            y_pred = [1 if get_func_id(func) in predicted_func_ids else 0 for func in all_funcs]

            precision = precision_score(y_true, y_pred)
            recall = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)
            print('CVE:', cve)
            # print('number of predicted', len(predicted_funcs))
            print(f"Precision: {precision:.4f}")
            print(f"Recall: {recall:.4f}")
            print(f"F1 Score: {f1:.4f}")
            print('Done', cve)
            # store precision, recall, f1, predicted_func_ids, gt_funcs_4_cve, funcs, scores
            results[cve] ={
                'precision': precision, 
                'recall': recall, 
                'f1': f1,
                'predicted_func_ids': predicted_func_ids,
                'gt_funcs_4_cve': gt_funcs_4_cve,
                'funcs': funcs,
                'scores': scores
                }
            with open(result_file, 'w') as f:
                json.dump(results, f)
def evaluation_summary(result_folder):
    processed = set()
    precision = 0
    recall = 0
    cnt_empty_prediction = 0
    all_count = 0
    ground_truth_funcs = read_filtered_funcs()

    tp = []
    fp = []
    fn = []
    
    tp_gt = []
    fp_gt = []
    fn_gt = []

    for result_file in os.listdir(result_folder):
        with open(result_folder+'/'+result_file) as f:
            results = json.load(f)

            cve = result_file.replace('top000_', '').replace('.json', '')
            value = results
            if cve not in ground_truth_funcs:
                continue
            if cve in processed:
                continue
            
            gt_funcs_4_cve = [get_func_id(each_func) for each_func in ground_truth_funcs[cve]]
            positive = value['predicted_func_ids']
            postive_func_names = [each.split('@')[0] for each in positive]
            target_funcs = get_gt_funcs_from_cve(cve)
 
            processed.add(cve)
            # print('############', cve)
            
            all_count+=1
            for pos in positive:
                if pos in gt_funcs_4_cve:
                    tp.append(pos)
                else:
                    fp.append(pos)
                    # print(pos)
            # print('GT', gt_funcs_4_cve)
            for gt in gt_funcs_4_cve:
                if gt not in positive:
                    fn.append(gt)
            for pos_name in postive_func_names:
                if pos_name in target_funcs:
                    tp_gt.append(pos_name)
                else:
                    fp_gt.append(pos_name)
            for gt_func in target_funcs:
                if gt_func not in postive_func_names:
                    fn_gt.append(gt_func)


            if value['predicted_func_ids'] == []:
                cnt_empty_prediction += 1

    print('Empty predictions:', cnt_empty_prediction, '/', all_count)
    print('Tool fitting:')
    print('Average precision:', len(tp)/(len(tp)+len(fp)))
    print('Average recall:', len(tp)/(len(tp)+len(fn)))
    print('Ground truth:')
    print('Average precision:', len(tp_gt)/(len(tp_gt)+len(fp_gt)))
    print('Average recall:', len(tp_gt)/(len(tp_gt)+len(fn_gt)))

    # print('Average f1:', )